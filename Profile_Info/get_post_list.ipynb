{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Post List\n",
    "Given a list of accounts, get the links for the first 24 posts from the profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and Helper Functions\n",
    "\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import os\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import random\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "\n",
    "def login_to_instagram(driver, username, password):\n",
    "    driver.get('https://www.instagram.com/accounts/login/')\n",
    "    time.sleep(8)\n",
    "    \n",
    "    # Locate the username and password fields and enter your credentials\n",
    "    username_input = driver.find_element(\"name\", \"username\")\n",
    "    password_input = driver.find_element(\"name\", \"password\")\n",
    "    \n",
    "    username_input.send_keys(username)\n",
    "    password_input.send_keys(password)\n",
    "    \n",
    "    # Press the login button\n",
    "    login_button = driver.find_element(\"xpath\", '//*[@id=\"loginForm\"]/div/div[3]/button')\n",
    "    login_button.click()\n",
    "    \n",
    "    print(f\"Successfully logged in using {username} account!\")\n",
    "    time.sleep(8)\n",
    "\n",
    "# Function to navigate to the profile page\n",
    "def navigate_to_profile(driver, account_name):\n",
    "    try:\n",
    "        driver.get(f\"https://www.instagram.com/{account_name}/\")\n",
    "        print(f\"Navigated to profile: {account_name}\")\n",
    "        time.sleep(random.uniform(3,5))\n",
    "    except Exception as e:\n",
    "        print(f\"Error navigating to {account_name}: {e}\")\n",
    "\n",
    "def check_account_private(account_name, driver):\n",
    "    tracker_file = \"post_link_tracker.csv\"\n",
    "    \n",
    "    try:\n",
    "        # Locate the \"This account is private\" element\n",
    "        account_priv_element = driver.find_element(By.XPATH, \n",
    "            \"//span[contains(text(), 'This account is private')]\")\n",
    "        account_priv_message = account_priv_element.text\n",
    "\n",
    "        if account_priv_message == \"This account is private\":\n",
    "            print(\"This account is private. Skipping.\")\n",
    "\n",
    "            # Check if file exists, create with header if not\n",
    "            file_exists = os.path.exists(tracker_file)\n",
    "            with open(tracker_file, mode='a', encoding='utf-8', newline='') as csvfile:\n",
    "                writer = csv.writer(csvfile)\n",
    "                if not file_exists:\n",
    "                    writer.writerow([\"Account Name\", \"Timestamp\", \"Comments\"])  # Write header if new file\n",
    "                writer.writerow([account_name, datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Account is Private\"])\n",
    "                print(f\"{account_name} is private, added to {tracker_file}\")\n",
    "\n",
    "            return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Account should be public.\")\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_links(driver, account_name):\n",
    "    csv_filepath = f\"post_links/{account_name}_post_links.csv\"\n",
    "\n",
    "    # Load existing reel URLs from the CSV to avoid duplicates\n",
    "    links_found = set()\n",
    "    total_urls = 0  # Counter to keep track of the total URLs read\n",
    "\n",
    "    if os.path.exists(csv_filepath):\n",
    "        print(f\"CSV for {account_name} exists: {csv_filepath}. Reading in existing URLs.\")\n",
    "        with open(csv_filepath, 'r', newline='', encoding='utf-8') as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            next(reader)\n",
    "            for row in reader:\n",
    "                links_found.add(row[0])\n",
    "                total_urls += 1  # Increment the counter for each URL read\n",
    "            print(f\"{total_urls} found for {account_name} from CSV.\")\n",
    "    else:\n",
    "        navigate_to_profile(driver, account_name)\n",
    "        if check_account_private(account_name, driver):\n",
    "            return\n",
    "    \n",
    "        print(f\"No existing CSV for {account_name} found. Creating CSV.\")\n",
    "        with open(csv_filepath, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames = [\"url\"])\n",
    "            writer.writeheader()\n",
    "\n",
    "    if total_urls >= 24:\n",
    "        print(f\"{total_urls} total links already found for {account_name} in csv. Skipping.\")\n",
    "        with open(\"post_link_tracker.csv\", mode='a', encoding='utf-8', newline='') as csvfile:\n",
    "                            writer = csv.writer(csvfile)\n",
    "                            writer.writerow([account_name, datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), f\"{total_urls} urls collected\"])\n",
    "        return\n",
    "    \n",
    "    navigate_to_profile(driver, account_name)\n",
    "\n",
    "    with open(csv_filepath, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        try:\n",
    "            WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.XPATH, '//a[contains(@href, \"/p/\") or contains(@href, \"/reel/\")]')))\n",
    "        except TimeoutException:\n",
    "            print(\"Timeout while waiting for initial post links.\")\n",
    "            return\n",
    "\n",
    "        # Variables for scrolling control\n",
    "        scroll_increment = 3000  # Scroll increment\n",
    "        last_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
    "        last_reel_count = len(driver.find_elements(By.XPATH, '//a[contains(@href, \"/p/\") or contains(@href, \"/reel/\")]'))\n",
    "\n",
    "        # Continue scrolling until reaching the bottom of the page\n",
    "        retry_count = 0\n",
    "        max_retries = 3  \n",
    "        new_links_found = False\n",
    "\n",
    "        while total_urls < 24:\n",
    "            # Find all reel links on the page\n",
    "            posts_found = driver.find_elements(By.XPATH, '//a[contains(@href, \"/p/\") or contains(@href, \"/reel/\")]')\n",
    "            current_post_count = len(posts_found)\n",
    "            print(f\"Found {current_post_count} links on webpage, {total_urls} collected in running total.\")\n",
    "\n",
    "            for post in posts_found:\n",
    "                post_url = post.get_attribute('href')\n",
    "                \n",
    "                # If it's a new link (not in CSV), add it to the CSV and the set\n",
    "                if post_url not in links_found:\n",
    "                    links_found.add(post_url)  # Add to running set\n",
    "                    writer.writerow([post_url])    # Write directly to CSV\n",
    "                    new_links_found = True\n",
    "                    print(f\"New link added to CSV: {post_url}\")\n",
    "                    total_urls += 1\n",
    "\n",
    "                    if total_urls == 24:\n",
    "                        with open(\"post_link_tracker.csv\", mode='a', encoding='utf-8', newline='') as csvfile:\n",
    "                            writer = csv.writer(csvfile)\n",
    "                            writer.writerow([account_name, datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), f\"{total_urls} urls collected\"])\n",
    "                        print(f\"{total_urls} urls found for {account_name}. Updated Tracker, Moving to next profile.\")\n",
    "                        return\n",
    "\n",
    "            # Scroll down to load more reels\n",
    "            driver.execute_script(\"window.scrollBy(0, arguments[0]);\", scroll_increment)\n",
    "            time.sleep(3)  # Pause to allow new content to load\n",
    "            new_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
    "            print(f\"Just tried scrolling: last height: {last_height}  new height: {new_height} (should be different)\")\n",
    "\n",
    "            if new_height == last_height:\n",
    "                time.sleep(1)\n",
    "                driver.execute_script(\"window.scrollBy(0, arguments[0]);\", scroll_increment)\n",
    "                time.sleep(2)  # Pause to allow new content to load\n",
    "                # Check the new scroll height and compare it to the last height\n",
    "                new_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
    "                print(f\"Just tried scrolling: last height: {last_height}  new height: {new_height} (should be different)\")\n",
    "\n",
    "            # If no new links are found, attempt to scroll again\n",
    "            if not new_links_found:\n",
    "                print(f\"No new reel links found, trying again... last height: {last_height}  new height: {new_height}\")\n",
    "\n",
    "            # If new height is equal to the last height and no new links were found, stop scrolling\n",
    "            if new_height == last_height and current_post_count == last_reel_count:\n",
    "                retry_count += 1\n",
    "                print(f\"Attempt {retry_count}/{max_retries}: No new content loaded. Stopping.\")\n",
    "                if retry_count >= max_retries:\n",
    "                    print(f\"Max retries reached. Stopping the scraping process.\")\n",
    "                    with open(\"post_link_tracker.csv\", mode='a', encoding='utf-8', newline='') as csvfile:\n",
    "                            writer = csv.writer(csvfile)\n",
    "                            writer.writerow([account_name, datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), f\"{total_urls} urls collected\"])\n",
    "                    print(f\"{total_urls} urls found for {account_name}. Updated Tracker, Moving to next profile.\")\n",
    "                    break\n",
    "\n",
    "            # Update last height and last reel count for the next iteration\n",
    "            last_height = new_height\n",
    "            last_reel_count = current_post_count\n",
    "            print(f\"Updating scroll height for this attempt: last height: {last_height}  new height: {new_height}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "account_names_df = pd.read_csv(\"aggregated_account_counts.csv\")\n",
    "account_names_list = list(account_names_df['Account Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10842"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(account_names_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_profiles(account_names_list, login_user):\n",
    "    tracker_file = \"post_link_tracker.csv\"\n",
    "\n",
    "    visited_profiles = []\n",
    "    if os.path.exists(tracker_file) and os.path.getsize(tracker_file) > 0:\n",
    "        with open(tracker_file, mode=\"r\", encoding=\"utf-8\") as file:\n",
    "            csv_reader = csv.DictReader(file)\n",
    "            visited_profiles = [row[\"Account Name\"] for row in csv_reader if \"Account Name\" in row and row[\"Account Name\"]]\n",
    "\n",
    "    else:\n",
    "        with open(tracker_file, mode='w', encoding='utf-8', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow([\"Account Name\", \"Timestamp\", \"Comments\"])  # Write header if file doesn't exist\n",
    "\n",
    "    driver = webdriver.Chrome()\n",
    "    login_to_instagram(driver, login_user, 'wellesley')  \n",
    "    time.sleep(5)\n",
    "\n",
    "    for account_name in account_names_list:\n",
    "        print(f\"----- Processing {account_name} -----\")\n",
    "        if account_name in visited_profiles:\n",
    "            print(f\"Already collected lists from {account_name}. Skipping\")\n",
    "            continue\n",
    "        get_top_links(driver, account_name)\n",
    "\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully logged in using bingbongding2024 account!\n",
      "----- Processing lavina.hk -----\n",
      "Already collected lists from lavina.hk. Skipping\n",
      "----- Processing catharina.cheung -----\n",
      "Already collected lists from catharina.cheung. Skipping\n",
      "----- Processing supwayhk -----\n",
      "Already collected lists from supwayhk. Skipping\n",
      "----- Processing dji_hkmacau -----\n",
      "Already collected lists from dji_hkmacau. Skipping\n",
      "----- Processing thepeninsulaboutique -----\n",
      "Already collected lists from thepeninsulaboutique. Skipping\n",
      "----- Processing doubleducks_official -----\n",
      "Already collected lists from doubleducks_official. Skipping\n",
      "----- Processing andrechiang_sg -----\n",
      "Already collected lists from andrechiang_sg. Skipping\n",
      "----- Processing lea.cantalloube.hk -----\n",
      "Already collected lists from lea.cantalloube.hk. Skipping\n",
      "----- Processing lighterluxe -----\n",
      "Already collected lists from lighterluxe. Skipping\n",
      "----- Processing dunfallandy.house -----\n",
      "Already collected lists from dunfallandy.house. Skipping\n",
      "----- Processing lovelanguage_cakery -----\n",
      "Already collected lists from lovelanguage_cakery. Skipping\n",
      "----- Processing timeout_asia -----\n",
      "Already collected lists from timeout_asia. Skipping\n",
      "----- Processing mo_landmarkhk -----\n",
      "Already collected lists from mo_landmarkhk. Skipping\n",
      "----- Processing eatwithada -----\n",
      "Already collected lists from eatwithada. Skipping\n",
      "----- Processing soulfresh_hk -----\n",
      "Already collected lists from soulfresh_hk. Skipping\n",
      "----- Processing cherrychanster -----\n",
      "Already collected lists from cherrychanster. Skipping\n",
      "----- Processing lemonadebyke -----\n",
      "Already collected lists from lemonadebyke. Skipping\n",
      "----- Processing ascend.projects -----\n",
      "Already collected lists from ascend.projects. Skipping\n",
      "----- Processing gingko2005 -----\n",
      "Already collected lists from gingko2005. Skipping\n",
      "----- Processing silverlai18 -----\n",
      "Already collected lists from silverlai18. Skipping\n",
      "----- Processing charmaintsoi -----\n",
      "Already collected lists from charmaintsoi. Skipping\n",
      "----- Processing streamsgallery -----\n",
      "Already collected lists from streamsgallery. Skipping\n",
      "----- Processing tindlefoods -----\n",
      "Navigated to profile: tindlefoods\n",
      "Account should be public.\n",
      "No existing CSV for tindlefoods found. Creating CSV.\n",
      "Navigated to profile: tindlefoods\n",
      "Found 12 links on webpage, 0 collected in running total.\n",
      "New link added to CSV: https://www.instagram.com/fitgreenmind/reel/CpVvUzMqbMo/\n",
      "New link added to CSV: https://www.instagram.com/tindlefoods/reel/Crs7wKkqLyW/\n",
      "New link added to CSV: https://www.instagram.com/nomzallday/reel/DFx29k1RlRd/\n",
      "New link added to CSV: https://www.instagram.com/plntburger/p/DFqJzzkR8U2/\n",
      "New link added to CSV: https://www.instagram.com/plntburger/p/DFfr9ZAxmbY/\n",
      "New link added to CSV: https://www.instagram.com/tindlefoods/p/DE7rpdBqphv/\n",
      "New link added to CSV: https://www.instagram.com/tindlefoods/reel/DEzQiGayPy8/\n",
      "New link added to CSV: https://www.instagram.com/tindlefoods/reel/DEfSaKPM2VU/\n",
      "New link added to CSV: https://www.instagram.com/plntburger/p/DEaONnsRl_1/\n",
      "New link added to CSV: https://www.instagram.com/plntburger/p/DEP8fjoRVhV/\n",
      "New link added to CSV: https://www.instagram.com/tindlefoods/p/DDpUFd_SUtU/\n",
      "New link added to CSV: https://www.instagram.com/tindlefoods/p/DDoeXoWyDc1/\n",
      "Just tried scrolling: last height: 1918  new height: 3165 (should be different)\n",
      "Updating scroll height for this attempt: last height: 3165  new height: 3165\n",
      "Found 24 links on webpage, 12 collected in running total.\n",
      "New link added to CSV: https://www.instagram.com/tindlefoods/reel/DDZdulVy4nQ/\n",
      "New link added to CSV: https://www.instagram.com/tindlefoods/p/DDM7DAMutBF/\n",
      "New link added to CSV: https://www.instagram.com/tindlefoods/reel/DChL9b1umic/\n",
      "New link added to CSV: https://www.instagram.com/mrcharlies/reel/DCXdhn1Acpu/\n",
      "New link added to CSV: https://www.instagram.com/tindlefoods/p/DCW_w9QMuB1/\n",
      "New link added to CSV: https://www.instagram.com/veganhackspod/reel/DCUp1aqSE9-/\n",
      "New link added to CSV: https://www.instagram.com/tindlefoods/p/DCR8DOSu8HC/\n",
      "New link added to CSV: https://www.instagram.com/tindlefoods/p/DBwlsOjuFjr/\n",
      "New link added to CSV: https://www.instagram.com/growplantbased/reel/DA3VwYfJOlx/\n",
      "New link added to CSV: https://www.instagram.com/tindlefoods/p/DAyHBS6uhE9/\n",
      "New link added to CSV: https://www.instagram.com/mrcharlies/reel/DAhfG_ovv0_/\n",
      "New link added to CSV: https://www.instagram.com/tindlefoods/p/DAgRRKqOneg/\n",
      "24 urls found for tindlefoods. Updated Tracker, Moving to next profile.\n",
      "----- Processing findingdaisyhk -----\n",
      "Navigated to profile: findingdaisyhk\n",
      "Account should be public.\n",
      "No existing CSV for findingdaisyhk found. Creating CSV.\n",
      "Navigated to profile: findingdaisyhk\n",
      "Found 12 links on webpage, 0 collected in running total.\n",
      "New link added to CSV: https://www.instagram.com/findingdaisyhk/p/DBONHu7NtqQ/\n",
      "New link added to CSV: https://www.instagram.com/findingdaisyhk/p/DATW6SyABsQ/\n",
      "New link added to CSV: https://www.instagram.com/findingdaisyhk/p/DABOe2LtGWA/\n",
      "New link added to CSV: https://www.instagram.com/findingdaisyhk/p/C_0IxgLC99K/\n",
      "New link added to CSV: https://www.instagram.com/findingdaisyhk/p/C_fXBmxh3BD/\n",
      "New link added to CSV: https://www.instagram.com/findingdaisyhk/p/C--K9kuMQe9/\n",
      "New link added to CSV: https://www.instagram.com/findingdaisyhk/p/C-r7qDHoTDa/\n",
      "New link added to CSV: https://www.instagram.com/findingdaisyhk/p/C-H4PDesEQn/\n",
      "New link added to CSV: https://www.instagram.com/findingdaisyhk/p/C94yapTSVxT/\n",
      "New link added to CSV: https://www.instagram.com/findingdaisyhk/p/C9o_iVlNaZs/\n",
      "New link added to CSV: https://www.instagram.com/findingdaisyhk/p/C9XLJtEMg88/\n",
      "New link added to CSV: https://www.instagram.com/findingdaisyhk/p/C9HZUTrStpZ/\n",
      "Just tried scrolling: last height: 1882  new height: 3129 (should be different)\n",
      "Updating scroll height for this attempt: last height: 3129  new height: 3129\n",
      "Found 24 links on webpage, 12 collected in running total.\n",
      "New link added to CSV: https://www.instagram.com/findingdaisyhk/reel/C6GgSWpylcX/\n",
      "New link added to CSV: https://www.instagram.com/findingdaisyhk/p/C58P4fkrxeC/\n",
      "New link added to CSV: https://www.instagram.com/findingdaisyhk/p/C5x5hawPLxi/\n",
      "New link added to CSV: https://www.instagram.com/findingdaisyhk/p/C5f7A8SvO78/\n",
      "New link added to CSV: https://www.instagram.com/findingdaisyhk/reel/C5Vjz5iSsh9/\n",
      "New link added to CSV: https://www.instagram.com/findingdaisyhk/p/C5QbMA8MDav/\n",
      "New link added to CSV: https://www.instagram.com/findingdaisyhk/p/C5BAeoASNLa/\n",
      "New link added to CSV: https://www.instagram.com/findingdaisyhk/p/C47xkNfrcVw/\n",
      "New link added to CSV: https://www.instagram.com/findingdaisyhk/p/C40NVAqyk7_/\n",
      "New link added to CSV: https://www.instagram.com/findingdaisyhk/p/C4u-jFRNdNV/\n",
      "New link added to CSV: https://www.instagram.com/findingdaisyhk/p/C4qGy6NvxDn/\n",
      "New link added to CSV: https://www.instagram.com/findingdaisyhk/p/C4kmN8Ni-dN/\n",
      "24 urls found for findingdaisyhk. Updated Tracker, Moving to next profile.\n",
      "----- Processing zlism -----\n",
      "Navigated to profile: zlism\n",
      "Account should be public.\n",
      "No existing CSV for zlism found. Creating CSV.\n",
      "Navigated to profile: zlism\n",
      "Found 12 links on webpage, 0 collected in running total.\n",
      "New link added to CSV: https://www.instagram.com/zlism/p/C40V0uBvYhw/\n",
      "New link added to CSV: https://www.instagram.com/zlism/p/C40Vw0zvEIQ/\n",
      "New link added to CSV: https://www.instagram.com/zlism/p/C40Vrniv3RO/\n",
      "New link added to CSV: https://www.instagram.com/zlism/p/DEPxU4MvxnP/\n",
      "New link added to CSV: https://www.instagram.com/tommy_knuts/p/DEFu16usdrp/\n",
      "New link added to CSV: https://www.instagram.com/zlism/p/DD7AtNbv5me/\n",
      "New link added to CSV: https://www.instagram.com/zlism/p/DDGbrkjh3IA/\n",
      "New link added to CSV: https://www.instagram.com/zlism/p/DCoKtKfPD6y/\n",
      "New link added to CSV: https://www.instagram.com/zlism/p/DB8O9eNvi3f/\n",
      "New link added to CSV: https://www.instagram.com/zlism/p/DBc-5jQvMYV/\n",
      "New link added to CSV: https://www.instagram.com/zlism/p/DBa6EhvPg29/\n",
      "New link added to CSV: https://www.instagram.com/zlism/reel/C_vIIKkPJya/\n",
      "Just tried scrolling: last height: 1934  new height: 3181 (should be different)\n",
      "Updating scroll height for this attempt: last height: 3181  new height: 3181\n",
      "Found 24 links on webpage, 12 collected in running total.\n",
      "New link added to CSV: https://www.instagram.com/jazz.org.hk/p/C_ab6iMPbTB/\n",
      "New link added to CSV: https://www.instagram.com/zlism/p/C_PoQUYvzcv/\n",
      "New link added to CSV: https://www.instagram.com/zlism/reel/C-c77j4P0Zo/\n",
      "New link added to CSV: https://www.instagram.com/lojeljourneys/reel/C-MLNq5Pnqn/\n",
      "New link added to CSV: https://www.instagram.com/zlism/p/C9mgDj-PDTy/\n",
      "New link added to CSV: https://www.instagram.com/sportb_officiel/reel/C8boiOov3Cr/\n",
      "New link added to CSV: https://www.instagram.com/zlism/reel/C74CsxKvRpq/\n",
      "New link added to CSV: https://www.instagram.com/zlism/p/C7Oouf6NXjj/\n",
      "New link added to CSV: https://www.instagram.com/zlism/p/C7L2WHwvj_M/\n",
      "New link added to CSV: https://www.instagram.com/zlism/p/C6tgrR4vXYR/\n",
      "New link added to CSV: https://www.instagram.com/zlism/p/C6X1xZwvM8n/\n",
      "New link added to CSV: https://www.instagram.com/zlism/reel/C5sOnHrvbQ9/\n",
      "24 urls found for zlism. Updated Tracker, Moving to next profile.\n",
      "----- Processing test_tohk -----\n",
      "Navigated to profile: test_tohk\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[113], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmultiple_profiles\u001b[49m\u001b[43m(\u001b[49m\u001b[43maccount_names_list\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbingbongding2024\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[109], line 24\u001b[0m, in \u001b[0;36mmultiple_profiles\u001b[0;34m(account_names_list, login_user)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAlready collected lists from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccount_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Skipping\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     \u001b[43mget_top_links\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccount_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m driver\u001b[38;5;241m.\u001b[39mquit()\n",
      "Cell \u001b[0;32mIn[111], line 18\u001b[0m, in \u001b[0;36mget_top_links\u001b[0;34m(driver, account_name)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_urls\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m found for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccount_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from CSV.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 18\u001b[0m     \u001b[43mnavigate_to_profile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccount_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m check_account_private(account_name, driver):\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[50], line 37\u001b[0m, in \u001b[0;36mnavigate_to_profile\u001b[0;34m(driver, account_name)\u001b[0m\n\u001b[1;32m     35\u001b[0m     driver\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.instagram.com/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccount_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNavigated to profile: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccount_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muniform\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError navigating to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccount_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "multiple_profiles(account_names_list[10:], \"bingbongding2024\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".thesis_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
